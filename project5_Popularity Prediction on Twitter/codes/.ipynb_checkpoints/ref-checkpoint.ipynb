{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/188136 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Models First\n",
      "###\n",
      "# gohawks:\n",
      "###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188136/188136 [00:18<00:00, 10281.61it/s]\n",
      "  0%|          | 0/259024 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###\n",
      "# nfl:\n",
      "###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 259024/259024 [00:23<00:00, 11085.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###\n",
      "# superbowl:\n",
      "###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1348767/1348767 [03:38<00:00, 6175.61it/s]\n",
      "100%|██████████| 365/365 [00:00<00:00, 8301.72it/s]\n",
      "100%|██████████| 730/730 [00:00<00:00, 9763.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test data\n",
      "Predicting number of tweets for file sample10_period3.txt\n",
      "Predictions for the 5 hour window are: [ 15.6202574    7.09296663  10.63803572   0.79901979  -6.0463673 ]\n",
      "Prediction errors are: 54.9792175506\n",
      "Predicting number of tweets for file sample1_period1.txt\n",
      "Predictions for the 5 hour window are: [-31.43287436  -9.19346291  36.87217555  12.6683037   32.93661493]\n",
      "Prediction errors are: 110.229848617\n",
      "Predicting number of tweets for file sample2_period2.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 212273/212273 [00:16<00:00, 12761.06it/s]\n",
      " 19%|█▉        | 704/3638 [00:00<00:00, 7037.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the 5 hour window are: [-413164.28422589 -371910.05692859 -494440.30252183 -395258.06171855\n",
      " -480072.10706794]\n",
      "Prediction errors are: 471905.362493\n",
      "Predicting number of tweets for file sample3_period3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 3628/3638 [00:00<00:00, 9681.17it/s]\n",
      "100%|██████████| 1646/1646 [00:00<00:00, 11570.86it/s]\n",
      "  0%|          | 0/2059 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the 5 hour window are: [-109.52969612    0.94110755  -65.25657106 -279.49045782    1.83490791]\n",
      "Prediction errors are: 727.700141908\n",
      "Predicting number of tweets for file sample4_period1.txt\n",
      "Predictions for the 5 hour window are: [ 77.01307327 -17.2248619  -40.10264019 -82.78064712 -46.42164035]\n",
      "Prediction errors are: 267.303343257\n",
      "Predicting number of tweets for file sample5_period1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2059/2059 [00:00<00:00, 13206.84it/s]\n",
      "  0%|          | 558/205554 [00:00<00:36, 5575.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the 5 hour window are: [-113.62915959 -314.13982053 -120.38835974  -97.20123714 -116.96064572]\n",
      "Prediction errors are: 495.863844546\n",
      "Predicting number of tweets for file sample6_period2.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205554/205554 [00:17<00:00, 11844.79it/s]\n",
      "100%|██████████| 528/528 [00:00<00:00, 5753.13it/s]\n",
      "100%|██████████| 229/229 [00:00<00:00, 6324.42it/s]\n",
      "  0%|          | 0/11311 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the 5 hour window are: [ -335277.14869096   136885.32360838  3391851.59924414  2849580.62612228\n",
      "  1910258.7574437 ]\n",
      "Prediction errors are: 1689028.09102\n",
      "Predicting number of tweets for file sample7_period3.txt\n",
      "Predictions for the 5 hour window are: [-81.0689205  -63.00961061 -13.97155776   8.62436914 -22.5929161 ]\n",
      "Prediction errors are: 115.003727166\n",
      "Predicting number of tweets for file sample8_period1.txt\n",
      "Predictions for the 5 hour window are: [-38.76508767 -62.50521337 -50.66003519 -34.24180419]\n",
      "Prediction errors are: 91.5430351041\n",
      "Predicting number of tweets for file sample9_period2.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11311/11311 [00:00<00:00, 12963.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the 5 hour window are: [-352981.70702158 -402228.97930408 -457532.89555471 -506424.51255364\n",
      " -458654.59951995]\n",
      "Prediction errors are: 437480.938791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_curve\n",
    "import sklearn.metrics as smet\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def in_washington(location):\n",
    "    white_list = [\n",
    "        \"seattle\",\n",
    "        \"washington\",\n",
    "        \"wa\",\n",
    "        \"kirkland\"\n",
    "    ]\n",
    "\n",
    "    black_list = [\n",
    "        \"dc\",\n",
    "        \"d.c.\",\n",
    "        \"d.c.\"\n",
    "    ]\n",
    "\n",
    "    flag = False\n",
    "    location = location.split()\n",
    "\n",
    "    for s in white_list:\n",
    "        if s in location:\n",
    "            flag = True\n",
    "            break\n",
    "\n",
    "    for s in black_list:\n",
    "        if s in location:\n",
    "            flag = False\n",
    "            break\n",
    "\n",
    "    return flag\n",
    "\n",
    "def in_mas(location):\n",
    "    white_list = [\n",
    "        \"ma\",\n",
    "        \"massachusetts\",\n",
    "        \"boston\",\n",
    "        \"worcester\",\n",
    "        \"salem\",\n",
    "        \"plymouth\",\n",
    "        \"springfield\",\n",
    "        \"arlington\",\n",
    "        \"scituate\",\n",
    "        \"northampton\"\n",
    "    ]\n",
    "\n",
    "    location = location.split()\n",
    "\n",
    "    black_list = [\n",
    "        \"ohio\",\n",
    "    ]\n",
    "    flag = False\n",
    "\n",
    "    for s in white_list:\n",
    "        if s in location:\n",
    "            flag = True\n",
    "            break\n",
    "\n",
    "    for s in black_list:\n",
    "        if s in location:\n",
    "            flag = False\n",
    "            break\n",
    "\n",
    "    return flag\n",
    "\n",
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        doc = re.sub('[,.-:/()?{}*$#&]', ' ', doc)\n",
    "        doc = ''.join(ch for ch in doc if ch not in string.punctuation)\n",
    "        doc = ''.join(ch for ch in doc if ord(ch) < 128)\n",
    "        doc = doc.lower()\n",
    "        words = doc.split()\n",
    "        words = [word for word in words if word not in text.ENGLISH_STOP_WORDS]\n",
    "\n",
    "        return [\n",
    "            self.snowball_stemmer.stem(word) for word in words\n",
    "        ]\n",
    "\n",
    "def get_vectorizer():\n",
    "    return CountVectorizer(\n",
    "        tokenizer=StemTokenizer(),\n",
    "        lowercase=True,\n",
    "        min_df = 2,\n",
    "        max_df = 0.99\n",
    "    )\n",
    "\n",
    "def get_tfid_transformer():\n",
    "    return TfidfTransformer(\n",
    "        norm='l2',\n",
    "        sublinear_tf=True\n",
    "    )\n",
    "\n",
    "def get_svd():\n",
    "    return TruncatedSVD(n_components=100)\n",
    "\n",
    "def print_statistics(actual, predicted):\n",
    "    print (\"Accuracy is \", smet.accuracy_score(actual, predicted) * 100)\n",
    "    print (\"Precision is \", smet.precision_score(actual, predicted, average='macro') * 100)\n",
    "    print (\"Recall is \", smet.recall_score(actual, predicted, average='macro') * 100)\n",
    "    print (\"Confusion Matrix is \", smet.confusion_matrix(actual, predicted))\n",
    "\n",
    "def plot_roc(actual, predicted, classifier_name):\n",
    "    x, y, _ = roc_curve(actual, predicted)\n",
    "    plt.plot(x, y, label=\"ROC Curve\")\n",
    "    plt.plot([0, 1], [0, 1])\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.2])\n",
    "\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('ROC Curves for ' + classifier_name + 'Classifier')\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    plt.savefig('plots/' + classifier_name + '.png', format='png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def classify(X, Y, classifier, cname):\n",
    "    b = 0.85 * X.shape[0]\n",
    "    X_train = X[:b, :]\n",
    "    Y_train = Y[:b]\n",
    "\n",
    "    X_test = X[b:, :]\n",
    "    Y_test = Y[b:]\n",
    "\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    predicted = classifier.predict(X_test)\n",
    "    predicted_probs = classifier.predict_proba(X_test)\n",
    "\n",
    "    print_statistics(Y_test, predicted)\n",
    "    plot_roc(Y_test, predicted_probs[:, 1], cname)\n",
    "\n",
    "print (\"Loading superbowl tweets\")\n",
    "lcount = 1348767\n",
    "\n",
    "with open(join('tweet_data', 'tweets_#superbowl.txt'), 'r') as f:\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i, line in tqdm(enumerate(f), total=lcount):\n",
    "        tweet_data = json.loads(line)\n",
    "        location = tweet_data.get(\"tweet\").get(\"user\").get(\"location\").lower()\n",
    "\n",
    "        if in_washington(location):\n",
    "            X.append(tweet_data.get(\"title\"))\n",
    "            Y.append(0)\n",
    "        elif in_mas(location):\n",
    "            X.append(tweet_data.get(\"title\"))\n",
    "            Y.append(1)\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            ('vectorize', get_vectorizer()),\n",
    "            ('tf-idf', get_tfid_transformer()),\n",
    "            ('svd', get_svd())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print (\"Computing the LSI representation of the dataset\")\n",
    "    X = pipeline.fit_transform(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    # Randomly shuffle data\n",
    "    indexes = range(X.shape[0])\n",
    "    random.shuffle(indexes)\n",
    "    indexes = indexes\n",
    "    X_ = X[indexes, :]\n",
    "    Y_ = Y[indexes]\n",
    "\n",
    "    print (\"Statistics of SVM classifier:\")\n",
    "    classify(X_, Y_, svm.SVC(kernel='linear', probability=True), \"SVM\")\n",
    "\n",
    "    print (\"Statistics of AdaBoost Classifier are\")\n",
    "    classify(X_, Y_, AdaBoostClassifier(), \"AdaBoost\")\n",
    "\n",
    "    print (\"Statistics of Random Forest Classifier are\")\n",
    "    classify(X_, Y_, RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), \"RandomForestClassifier\")\n",
    "\n",
    "    print (\"Statistics of Neural Network Classifier are\")\n",
    "    classify(X_, Y_, MLPClassifier(alpha=1), \"Neural Network Classifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
