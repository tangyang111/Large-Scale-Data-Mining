{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1:  extracting features from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate raw combine dataset\n",
    "files = ['tweet_data/tweets_#gohawks.txt', 'tweet_data/tweets_#gopatriots.txt', 'tweet_data/tweets_#nfl.txt', 'tweet_data/tweets_#patriots.txt', 'tweet_data/tweets_#sb49.txt', 'tweet_data/tweets_#superbowl.txt']\n",
    "\n",
    "with open('tweet_data/tweets_#combine.txt', 'w') as target:\n",
    "    for file in files:\n",
    "        with open(file, 'r') as cur_file:\n",
    "                for line in cur_file:\n",
    "                    target.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime, time\n",
    "import pytz\n",
    "\n",
    "\n",
    "# --------------------- preprocessing ----------------------- #\n",
    "# define paths\n",
    "files = ['tweet_data/tweets_#gohawks.txt', 'tweet_data/tweets_#gopatriots.txt', 'tweet_data/tweets_#nfl.txt', 'tweet_data/tweets_#patriots.txt', 'tweet_data/tweets_#sb49.txt', 'tweet_data/tweets_#superbowl.txt', 'tweet_data/tweets_#combine.txt']\n",
    "\n",
    "# calculate statistics of each hashtag\n",
    "def cal_statistics(file):\n",
    "    date = []\n",
    "    time = []\n",
    "    tweet_count = []\n",
    "    followers_count = []\n",
    "    retweet_count = []\n",
    "    url_count = []\n",
    "    author_time = {} # name+nick : date : set(time)\n",
    "    authors_count = [] \n",
    "    mentions_count = []\n",
    "    rank_score = []\n",
    "    hashtag_count = []\n",
    "    # extract data\n",
    "    with open(file, 'r') as cur_file:\n",
    "        for line in cur_file:\n",
    "            data = json.loads(line)\n",
    "            # date and time\n",
    "            timestamp = data['citation_date']\n",
    "            pst_tz = pytz.timezone('US/Pacific')\n",
    "            timestamp = str(datetime.datetime.fromtimestamp(int(timestamp), pst_tz))\n",
    "            date_split = timestamp[0:10].split('-')\n",
    "            cur_date = int(date_split[0]+date_split[1]+date_split[2])\n",
    "            date.append(cur_date)\n",
    "            cur_time = int(timestamp[11:13])\n",
    "            time.append(cur_time)\n",
    "            \n",
    "            tweet_count.append(1)\n",
    "            followers_count.append(data['author']['followers'])\n",
    "            retweet_count.append(data['metrics']['citations']['total'])\n",
    "            url_count.append(len(data['tweet']['entities']['urls']))\n",
    "            \n",
    "            # unique authors\n",
    "            author_name = data['author']['name']+'+'+data['author']['nick']\n",
    "            if author_name in author_time:\n",
    "                ori_ = author_time[author_name]\n",
    "                if cur_date in ori_:\n",
    "                    ori_times = ori_[cur_date] # set\n",
    "                    if cur_time in ori_times:\n",
    "                        authors_count.append(0)\n",
    "                    else:\n",
    "                        authors_count.append(1)\n",
    "                        ori_times.add(cur_time)\n",
    "                else:\n",
    "                    authors_count.append(1)\n",
    "                    new_times = set()\n",
    "                    new_times.add(cur_time)\n",
    "                    ori_[cur_date] = new_times\n",
    "            else:\n",
    "                authors_count.append(1)\n",
    "                new_times = set()\n",
    "                new_times.add(cur_time)\n",
    "                new_dates = {}\n",
    "                new_dates[cur_date] = new_times\n",
    "                author_time[author_name] = new_dates\n",
    "                \n",
    "            mentions_count.append(len(data['tweet']['entities']['user_mentions']))\n",
    "            rank_score.append(data['metrics']['ranking_score'])\n",
    "            hashtag_count.append(data['title'].count('#'))\n",
    "        df = pd.DataFrame({\n",
    "            'tweet' : tweet_count,\n",
    "            'date' : date,\n",
    "            'time' : time,\n",
    "            'followers' : followers_count,\n",
    "            'retweets' : retweet_count,\n",
    "            'urls' : url_count,\n",
    "            'authors' : authors_count,\n",
    "            'mentions' : mentions_count,\n",
    "            'ranking score' : rank_score,\n",
    "            'hashtags' : hashtag_count\n",
    "        }, columns = ['tweet', 'date', 'time', 'followers', 'retweets', 'urls', 'authors', 'mentions', 'ranking score', 'hashtags'])\n",
    "        df.to_csv('extracted_data/Q1.3_'+file[18:-4]+'.csv', index = False)\n",
    "\n",
    "# extract data from each hashtag\n",
    "for file in files:\n",
    "    cal_statistics(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2: aggregating data from step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime, time\n",
    "import pytz\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import RegressionResults\n",
    "\n",
    "\n",
    "# define paths\n",
    "files = ['extracted_data/Q1.3_#gohawks.csv', 'extracted_data/Q1.3_#gopatriots.csv', 'extracted_data/Q1.3_#nfl.csv', 'extracted_data/Q1.3_#patriots.csv', 'extracted_data/Q1.3_#sb49.csv', 'extracted_data/Q1.3_#superbowl.csv', 'extracted_data/Q1.3_#combine.csv']\n",
    "\n",
    "def load_and_process(file):\n",
    "    # process and groupby data\n",
    "    data = pd.read_csv(file)\n",
    "    data.columns = ['tweet', 'date', 'time', 'followers', 'retweets', 'urls', 'authors', 'mentions', 'ranking score', 'hashtags']\n",
    "    df = data.groupby(['date', 'time']).agg({'tweet' : np.sum, 'retweets' : np.sum, 'followers' : np.sum, 'urls' : np.sum, 'authors' : np.sum, 'mentions' : np.sum, 'ranking score' : np.sum, 'hashtags' : np.sum})\n",
    "    \n",
    "    # fill up non-exists hours with all zero data\n",
    "    app_rows = []\n",
    "    for i in range(1,len(df.index)):  \n",
    "        pre_date = df.index[i-1][0]\n",
    "        pre_hour = int(df.index[i-1][1])\n",
    "        cur_date = df.index[i][0]\n",
    "        cur_hour = int(df.index[i][1])\n",
    "        if (cur_hour < pre_hour):\n",
    "            cur_hour = cur_hour + 24\n",
    "        hour_diff = cur_hour - pre_hour\n",
    "        while (hour_diff > 1):\n",
    "            pre_hour = pre_hour + 1\n",
    "            if (pre_hour > 23):\n",
    "                pre_date = cur_date\n",
    "                app_rows.append({'tweet':0,'date':pre_date,'time':pre_hour-24,'followers':0,'retweets':0,'urls':0,'authors':0,'mentions':0,'ranking score':0,'hashtags':0})\n",
    "            else:\n",
    "                app_rows.append({'tweet':0,'date':pre_date,'time':pre_hour,'followers':0,'retweets':0,'urls':0,'authors':0,'mentions':0,'ranking score':0,'hashtags':0})\n",
    "            hour_diff = cur_hour - pre_hour\n",
    "    for row in app_rows:\n",
    "        data = data.append(row, ignore_index=True)\n",
    "    \n",
    "    df = data.groupby(['date', 'time']).agg({'date' : pd.Series.unique, 'time' : pd.Series.unique, 'tweet' : np.sum, 'retweets' : np.sum, 'followers' : np.sum, 'urls' : np.sum, 'authors' : np.sum, 'mentions' : np.sum, 'ranking score' : np.sum, 'hashtags' : np.sum})\n",
    "    df.to_csv('extracted_data/Q1.4_'+file[20:-4]+'.csv', index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3: seperating data according to date and time\n",
    "\n",
    "1. Before Feb. 1, 8:00 a.m.\n",
    "2. Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
    "3. After Feb. 1, 8:00 p.m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "def seperate(df):\n",
    "    periods = []\n",
    "    periods.append(df.query('date < 20150201 or (date == 20150201 and time < 8)'))\n",
    "    periods.append(df.query('date == 20150201 and time >= 8 and time <= 20'))\n",
    "    periods.append(df.query('date > 20150201 or (date == 20150201 and time > 20)'))\n",
    "    return periods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4: using 3 models to train and predict\n",
    "For each hashtag, report the average cross-validation errors for the 3 different models.\n",
    "Note that you should do the 90-10% splitting for each model within its specific time\n",
    "window.\n",
    "<br><br>Your evaluated error should be of the form |Npredicted - Nreal|.\n",
    "<br>MAE (mean of 10 absolute errors) for each piece and each model\n",
    "<br><br>\\- 6 hashtags\n",
    "<br>&emsp;&emsp;\\- 3 time pieces \n",
    "<br>&emsp;&emsp;&emsp;&emsp;\\- **3 models**\n",
    "<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\\- **10 folds**\n",
    "<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\\- **average cross-validation error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================#gohawks Before================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR average error: 375.53193355\n",
      "SVM average error: 255.007610994\n",
      "NN average error: 13361.9308246\n",
      "================#gohawks Between================\n",
      "LR average error: 5027.97300083\n",
      "SVM average error: 6778.95\n",
      "NN average error: 309720.228385\n",
      "================#gohawks After================\n",
      "LR average error: 25.5056065135\n",
      "SVM average error: 32.0916666667\n",
      "NN average error: 964.898719925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'#gohawks': {'After': {'LR': 25.505606513540648,\n",
       "   'NN': 964.89871992522728,\n",
       "   'SVM': 32.091666666666661},\n",
       "  'Before': {'LR': 375.53193354986973,\n",
       "   'NN': 13361.930824564242,\n",
       "   'SVM': 255.00761099365755},\n",
       "  'Between': {'LR': 5027.9730008306415,\n",
       "   'NN': 309720.22838460229,\n",
       "   'SVM': 6778.9499999999998}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================#gopatriots Before================\n",
      "LR average error: 17.744468454\n",
      "SVM average error: 14.4570824524\n",
      "NN average error: 219.960596792\n",
      "================#gopatriots Between================\n",
      "LR average error: 569.792956022\n",
      "SVM average error: 2007.05\n",
      "NN average error: 121055.046742\n",
      "================#gopatriots After================\n",
      "LR average error: 2.74936825056\n",
      "SVM average error: 4.73269230769\n",
      "NN average error: 12.5525082041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'#gopatriots': {'After': {'LR': 2.7493682505608059,\n",
       "   'NN': 12.552508204079718,\n",
       "   'SVM': 4.7326923076923091},\n",
       "  'Before': {'LR': 17.74446845398672,\n",
       "   'NN': 219.96059679213622,\n",
       "   'SVM': 14.45708245243129},\n",
       "  'Between': {'LR': 569.79295602207162,\n",
       "   'NN': 121055.04674219395,\n",
       "   'SVM': 2007.05}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================#nfl Before================\n",
      "LR average error: 119.813957842\n",
      "SVM average error: 189.861680761\n",
      "NN average error: 1549.81695715\n",
      "================#nfl Between================\n",
      "LR average error: 5224.18796855\n",
      "SVM average error: 6023.1\n",
      "NN average error: 270915.075258\n",
      "================#nfl After================\n",
      "LR average error: 108.621711387\n",
      "SVM average error: 592.968681319\n",
      "NN average error: 41971.1660532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'#nfl': {'After': {'LR': 108.62171138652306,\n",
       "   'NN': 41971.166053195855,\n",
       "   'SVM': 592.96868131868121},\n",
       "  'Before': {'LR': 119.81395784162035,\n",
       "   'NN': 1549.8169571514263,\n",
       "   'SVM': 189.86168076109939},\n",
       "  'Between': {'LR': 5224.187968552249,\n",
       "   'NN': 270915.07525767788,\n",
       "   'SVM': 6023.1000000000004}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================#patriots Before================\n",
      "LR average error: 252.338981366\n",
      "SVM average error: 291.941173362\n",
      "NN average error: 4010.66136503\n",
      "================#patriots Between================\n",
      "LR average error: 92370.6873622\n",
      "SVM average error: 27055.25\n",
      "NN average error: 1394211.67512\n",
      "================#patriots After================\n",
      "LR average error: 65.207327554\n",
      "SVM average error: 149.131868132\n",
      "NN average error: 52407.3925701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'#patriots': {'After': {'LR': 65.207327554049215,\n",
       "   'NN': 52407.392570127973,\n",
       "   'SVM': 149.13186813186812},\n",
       "  'Before': {'LR': 252.33898136556914,\n",
       "   'NN': 4010.6613650293634,\n",
       "   'SVM': 291.9411733615222},\n",
       "  'Between': {'LR': 92370.687362200508,\n",
       "   'NN': 1394211.67512057,\n",
       "   'SVM': 27055.25}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================#sb49 Before================\n",
      "LR average error: 46.7453583267\n",
      "SVM average error: 106.868181818\n",
      "NN average error: 13379.7401053\n",
      "================#sb49 Between================\n",
      "LR average error: 93722.963115\n",
      "SVM average error: 50906.6\n",
      "NN average error: 4515871.4363\n",
      "================#sb49 After================\n",
      "LR average error: 99.3238449435\n",
      "SVM average error: 323.721978022\n",
      "NN average error: 188587.110651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'#sb49': {'After': {'LR': 99.323844943525302,\n",
       "   'NN': 188587.11065142445,\n",
       "   'SVM': 323.72197802197809},\n",
       "  'Before': {'LR': 46.745358326665198,\n",
       "   'NN': 13379.740105339844,\n",
       "   'SVM': 106.86818181818182},\n",
       "  'Between': {'LR': 93722.963115047853,\n",
       "   'NN': 4515871.4362993222,\n",
       "   'SVM': 50906.599999999999}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================#superbowl Before================\n",
      "LR average error: 362.654304676\n",
      "SVM average error: 451.030761099\n",
      "NN average error: 29733.0479296\n",
      "================#superbowl Between================\n",
      "LR average error: 264251.105849\n",
      "SVM average error: 183828.15\n",
      "NN average error: 68241848.5829\n",
      "================#superbowl After================\n",
      "LR average error: 167.808673838\n",
      "SVM average error: 857.338461538\n",
      "NN average error: 799406.95589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'#superbowl': {'After': {'LR': 167.80867383798019,\n",
       "   'NN': 799406.95588994003,\n",
       "   'SVM': 857.33846153846173},\n",
       "  'Before': {'LR': 362.65430467583508,\n",
       "   'NN': 29733.047929626598,\n",
       "   'SVM': 451.03076109936575},\n",
       "  'Between': {'LR': 264251.1058492153,\n",
       "   'NN': 68241848.582850277,\n",
       "   'SVM': 183828.14999999999}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================#combine Before================\n",
      "LR average error: 732.172940919\n",
      "SVM average error: 1460.00761099\n",
      "NN average error: 290347.515165\n",
      "================#combine Between================\n",
      "LR average error: 160531.399686\n",
      "SVM average error: 201545.0\n",
      "NN average error: 4383803.73886\n",
      "================#combine After================\n",
      "LR average error: 436.631025448\n",
      "SVM average error: 8042.51593407\n",
      "NN average error: 534348.643774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'#combine': {'After': {'LR': 436.63102544760557,\n",
       "   'NN': 534348.64377445925,\n",
       "   'SVM': 8042.5159340659329},\n",
       "  'Before': {'LR': 732.17294091872293,\n",
       "   'NN': 290347.51516482123,\n",
       "   'SVM': 1460.0076109936574},\n",
       "  'Between': {'LR': 160531.39968610316,\n",
       "   'NN': 4383803.7388595426,\n",
       "   'SVM': 201545.0}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "def regression_analysis(file, periods):\n",
    "    # input: dataframes of 3 time pieces of a hashtag and the file name\n",
    "    titles = ['Before', 'Between', 'After']\n",
    "    res = {}\n",
    "    res[str(file[20:-4])] = {}\n",
    "    \n",
    "    for i in range(len(periods)):\n",
    "        print('================' + str(file[20:-4]) + ' ' + titles[i] + '================')\n",
    "        res[str(file[20:-4])][titles[i]] = {}\n",
    "        period = periods[i]\n",
    "        input_arr = []\n",
    "        for index in period.index:\n",
    "            input_arr.append(period.loc[index, 'tweet':'hashtags'].values)\n",
    "        input_arr.pop()\n",
    "        output_arr = period.loc[period.index[1]:, 'tweet'].values\n",
    "        errors = three_models_ten_folds_errors(input_arr, output_arr)\n",
    "        for key in errors:\n",
    "            print(key + ' average error: ' + str(errors[key]))\n",
    "            res[str(file[20:-4])][titles[i]][key] = errors[key]\n",
    "    return res\n",
    "\n",
    "\n",
    "def three_models_ten_folds_errors(input_arr, output_arr):\n",
    "    ave_error = {}\n",
    "    ave_error['LR'] = 0\n",
    "    ave_error['SVM'] = 0\n",
    "    ave_error['NN'] = 0\n",
    "    for model in ave_error:\n",
    "        MAE = []\n",
    "        kf = KFold(n_splits=10, shuffle=False)\n",
    "        for train_index, test_index in kf.split(input_arr):\n",
    "            train_in = [input_arr[i] for i in train_index]\n",
    "            test_in = [input_arr[i] for i in test_index]\n",
    "            train_out = [output_arr[i] for i in train_index]\n",
    "            test_out = [output_arr[i] for i in test_index]\n",
    "            test_pre = fit_predict(model, train_in, train_out, test_in)\n",
    "            MAE.append(mean_absolute_error(test_out, test_pre))\n",
    "        ave_error[model] = np.mean(MAE)\n",
    "    return ave_error\n",
    "\n",
    "\n",
    "def fit_predict(model, train_in, train_out, test_in):\n",
    "    if model == 'LR':\n",
    "        tr_in = []\n",
    "        for i in range(len(train_in)):\n",
    "            tr_in.append(train_in[i][:])\n",
    "            np.append(tr_in[len(tr_in) - 1], 1)\n",
    "        te_in = []\n",
    "        for i in range(len(test_in)):\n",
    "            te_in.append(test_in[i][:])\n",
    "            np.append(te_in[len(te_in) - 1], 1)\n",
    "        reg = sm.OLS(train_out, tr_in)\n",
    "        results = reg.fit()\n",
    "        return results.predict(te_in)\n",
    "    elif model == 'SVM':\n",
    "        reg = svm.SVC(gamma=6)\n",
    "        reg.fit(train_in, train_out)\n",
    "        return reg.predict(test_in)\n",
    "    elif model == 'NN':\n",
    "        reg = MLPRegressor(hidden_layer_sizes=(10, ), activation='relu')\n",
    "        reg.fit(train_in, train_out)\n",
    "        return reg.predict(test_in)\n",
    "\n",
    "for file in files:\n",
    "    df = load_and_process(file)\n",
    "    periods = seperate(df)\n",
    "    res = regression_analysis(file, periods)\n",
    "    display(res)\n",
    "    res_ = res[file[20:-4]]\n",
    "    titles = ['Before', 'Between', 'After']\n",
    "    res_LR = []\n",
    "    res_NN = []\n",
    "    res_SVM = []\n",
    "    for i in range(3):\n",
    "        cur_res = res_[titles[i]]\n",
    "        res_LR.append(cur_res['LR'])\n",
    "        res_NN.append(cur_res['NN'])\n",
    "        res_SVM.append(cur_res['SVM'])\n",
    "        \n",
    "    df = pd.DataFrame({\n",
    "        file[20:-4] : titles,\n",
    "        'Linear Regression' : res_LR,\n",
    "        'Neural Network' : res_NN,\n",
    "        'SVM' : res_SVM\n",
    "    }, columns = [file[20:-4], 'Linear Regression', 'Neural Network', 'SVM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
